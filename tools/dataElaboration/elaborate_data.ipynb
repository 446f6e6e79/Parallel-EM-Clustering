{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc6057d",
   "metadata": {},
   "source": [
    "# Parallel Computing Performance Analysis\n",
    "\n",
    "This notebook analyzes parallel computing performance metrics from experimental data, including execution time, speedup, and efficiency across different problem sizes and numbers of processes.\n",
    "\n",
    "## Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689f9fe0",
   "metadata": {},
   "source": [
    "### List of import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62bc21a",
   "metadata": {},
   "source": [
    "### Load the data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65853e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the process information data\n",
    "DATA_DIR = \"../../data\"\n",
    "df = pd.read_csv(DATA_DIR+'/algorithm_results/execution_info.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e8b55",
   "metadata": {},
   "source": [
    "# Calculation Overview\n",
    "\n",
    "This section computes parallel-performance metrics from the experimental data.\n",
    "\n",
    "- Dataset size: we define the `dataset_size` as the array of dimensions `[n_samples, n_features, n_clusters]`\n",
    "\n",
    "- Data grouping: mean execution time per `dataset_size` and `n_process` → `grouped`.\n",
    "\n",
    "- Speedup: for each `dataset_size`, take the execution time with `n_process == 1` as T₁ and compute `speedup = T₁ / execution_time`.\n",
    "\n",
    "- Efficiency: `efficiency = speedup / n_process`.\n",
    "\n",
    "- Results are stored in `metrics_df` and summarized as pivot tables: `execution_time_table`, `speedup_table`, `efficiency_table`.\n",
    "\n",
    "- Filtering: a subset of datasets is selected via `selected_datasets` / `filtered_df` for plotting.\n",
    "\n",
    "- Visualization: `fig_speedup` and `fig_efficiency` show measured curves together with ideal reference lines (ideal speedup and acceptable efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dataset number of processes and input size (defined by n_samples, n_features, n_clusters), then compute the mean of all remaining numeric columns (times)\n",
    "grouped = df.groupby(['n_process', 'n_samples', 'n_features', 'n_clusters']).mean().reset_index()\n",
    "\n",
    "# Apply metrics calculation to each problem size group\n",
    "metrics_df = grouped.groupby(['n_samples', 'n_features', 'n_clusters']).apply(compute_metrics).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabd49c",
   "metadata": {},
   "source": [
    "## Execution Time Table\n",
    "\n",
    "Shows how computation time changes when using different numbers of processes for various dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort processes in logical order\n",
    "process_order = sorted(metrics_df['n_process'].unique())\n",
    "\n",
    "execution_time_table = make_pivot(metrics_df, \n",
    "                                  value='compute_time', \n",
    "                                  index='n_process', \n",
    "                                  columns=['n_samples', 'n_features', 'n_clusters'],\n",
    "                                  process_order=process_order)\n",
    "\n",
    "\n",
    "print(\"Execution Time Table (seconds):\")\n",
    "display(execution_time_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71810e16",
   "metadata": {},
   "source": [
    "## Speedup Analysis Table\n",
    "\n",
    "This section creates a **pivot table** to visualize the average *speedup* achieved as a function of the number of processes (`n_process`) and the dataset size (`[n_samples, n_features, n_clusters]`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup_table = make_pivot(metrics_df,\n",
    "                           value='speedup',\n",
    "                           index='n_process',\n",
    "                           columns=['n_samples', 'n_features', 'n_clusters'], \n",
    "                           process_order=process_order)\n",
    "\n",
    "print(\"Speedup Table (T₁/Tₚ):\")\n",
    "display(speedup_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25502e91",
   "metadata": {},
   "source": [
    "## Efficiency Analysis\n",
    "\n",
    "This section builds a **pivot table** to display the average *efficiency* for different numbers of processes (`n_process`) and dataset sizes (`[n_samples, n_features, n_clusters]`).  \n",
    "An efficiency close to **1.0** indicates near-perfect scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5281e922-a0b7-46f9-9a77-b1936f4832b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_table = make_pivot(metrics_df, \n",
    "                                  value='efficiency', \n",
    "                                  index='n_process', \n",
    "                                  columns=['n_samples', 'n_features', 'n_clusters'],\n",
    "                                  process_order=process_order)\n",
    "\n",
    "print(\"Efficiency Table (Speedup/Processes):\")\n",
    "display(efficiency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daec056-f24e-4135-a2f9-be4c6711cb6e",
   "metadata": {},
   "source": [
    "### Efficiency Analysis\n",
    "\n",
    "Similar to what was done on the previous section, here we build two **pivot tables**, focusing on the **efficiency** of the **E-Step** and **M-Step** respectively. This allows for a more in depth analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c77e1-66c5-46d9-a0cd-0b642bfb2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_table = make_pivot(metrics_df, \n",
    "                                  value='e_step_efficiency', \n",
    "                                  index='n_process', \n",
    "                                  columns=['n_samples', 'n_features', 'n_clusters'],\n",
    "                                  process_order=process_order)\n",
    "\n",
    "print(\"Efficiency Table for E-Step:\")\n",
    "display(efficiency_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db87bf6a-a627-4877-9d56-d65ec2c0a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency_table = make_pivot(metrics_df, \n",
    "                                  value='m_step_efficiency', \n",
    "                                  index='n_process', \n",
    "                                  columns=['n_samples', 'n_features', 'n_clusters'],\n",
    "                                  process_order=process_order)\n",
    "\n",
    "print(\"Efficiency Table for M-Step:\")\n",
    "display(efficiency_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304ea85",
   "metadata": {},
   "source": [
    "## Speedup Visualization\n",
    "\n",
    "This section visualizes the *speedup* behavior for selected datasets using **Plotly**.  \n",
    "You can manually specify which datasets to include by editing the `selected_datasets` list.  \n",
    "The plot compares the measured speedup with the **ideal linear speedup** (represented by the dashed red line).  \n",
    "\n",
    "Each dataset is shown as a separate colored curve, allowing for an easy comparison of scalability across different problem sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7963fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual dataset selection:\n",
    "# - selected_n_samples, selected_n_features, selected_n_clusters: lists of dataset parameters to include in the plot.\n",
    "#   All lists must have the same length and be ordered consistently: \n",
    "#   the first element in selected_n_samples corresponds to the first element in selected_n_features \n",
    "#   and the first element in selected_n_clusters, and so on.\n",
    "\n",
    "selected_n_samples  = [10000000, 2500000, 1250000]\n",
    "selected_n_features = [50, 50, 50]          \n",
    "selected_n_clusters = [15, 15, 15]\n",
    "\n",
    "# Filter metrics_df for selected datasets\n",
    "filtered_df = metrics_df[\n",
    "    (metrics_df['n_samples'].isin(selected_n_samples)) &\n",
    "    (metrics_df['n_features'].isin(selected_n_features)) &\n",
    "    (metrics_df['n_clusters'].isin(selected_n_clusters))\n",
    "]\n",
    "\n",
    "# Create and show the speedup figure\n",
    "fig_speedup = plot_metrics(filtered_df, metric='speedup', fixed_parameters=['n_samples'])\n",
    "fig_speedup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edbc98",
   "metadata": {},
   "source": [
    "# Efficiency Visualization\n",
    "\n",
    "This section plots the *parallel efficiency* for the selected datasets using **Plotly**.  \n",
    "Each curve represents how efficiently computational resources are used as the number of processes increases.  \n",
    "\n",
    "The red dotted line marks an **acceptable efficiency threshold** at 0.75, helping to visually identify when performance begins to degrade.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40175b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Efficiency figure\n",
    "fig_efficiency = plot_metrics(filtered_df, metric='efficiency')\n",
    "# Show plot\n",
    "fig_efficiency.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402f991",
   "metadata": {},
   "source": [
    "## Accuracy Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cbd4a3",
   "metadata": {},
   "source": [
    "The `clustering_accuracy` function computes the accuracy of clustering results by comparing predicted labels with the true labels.  \n",
    "The function returns the accuracy as a **percentage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = DATA_DIR+\"algorithm_results/em_validation.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "acc = clustering_accuracy(df)\n",
    "print(f\"Clustering accuracy: {acc:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
