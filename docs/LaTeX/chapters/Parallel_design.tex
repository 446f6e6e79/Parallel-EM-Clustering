\section{Parallel Design}
Current state-of-the-art parallelization techniques for the EM algorithm primarily focus on data parallelism approaches, where
the dataset is partitioned across multiple processing units so that computations can be performed simultaneously on different subsets of the data.
\cite{altinigneli2013massively} present a GPU-based implementation of the EM algorithm that leverages the massive parallelism of GPUs to accelerate the computation of both the E-step and M-step using Asynchronous Model Update strategy.
Similarly, \cite{lee2016simple} propose a simple multithreaded version of the EM algorithm for finite mixture models.
\\Both approaches demonstrate significant speedups compared to traditional sequential implementations, making them suitable for large-scale data analysis tasks.

\subsection{Our Approach}
Building on the insights of the reviewed literature, our approach adopts a data-parallelism strategy to enhance the performance of the EM algorithm for GMMs.
The dataset is partitioned into equal chunks and distributed across multiple processing units, which perform computations on their respective data subsets.

\subsubsection*{E-step Parallelization}
The E-step is trivially parallelizable, as responsibilities for each data point are computed independently of each other.
Each processing unit can execute the standard E-step on its local data chunk, storing the results for the subsequent
M-step aggregation. This requires minimal modification from the sequential algorithm (Algorithm \ref{e_step_alg}) and involves little to no inter-process communication.

\subsubsection*{M-step Parallelization}
Differently, the M-step requires a more coordinated approach to ensure globally consistent parameters updates.
Each processing unit computes local accumulators for the sufficient statistics (i.e., effective counts, weighted sums for the means, and weighted squared differences for the variances) based on its local responsibilities.
These local results are then aggregated, in order to finalize the global parameter updates.

\subsubsection*{Clustering Assignment}
The clustering assignment step can be parallelized in a similar manner to the E-step.
Each processing unit determines the cluster assignments for its local data chunk using the final responsibilities from the preceding E-step.
Since the assignments are independent for each data point, this step requires no inter-process communication.
Once all units complete their local clustering assignments, the results can be gathered if needed for further analysis or output.

\subsubsection*{Performance Considerations}
Theoretically, this design reduces the computation times associated with the dataset size. Based on the computational complexity analysis in Section \ref{sec:complexity_analysis}, ideally the overall time complexity of the EM algorithm is reduced from $O(NKD)$ to $O\left(\frac{NKD}{P}\right)$, where $P$ is the number of processing units.
\\We assume that the number of clusters $K$ and dimensionality $D$ are significantly smaller than the number of data points $N$, which is a common characteristic of many real-world large-scale clustering applications.
Under this assumption, the computational cost is dominated by $N$, making our parallelization strategy particularly effective: distributing the data across processing units yields near-linear reductions in execution time.
Furthermore, this assumption ensures that the communication overhead during the M-step aggregation remains manageable, as the size of the parameters to be communicated between processing units depends only on $K$ and $D$.

\subsection{Hybrid Parallelism}
\subsubsection{Data dependencies}

\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.05}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Memory Location} & \multicolumn{3}{c|}{Earlier Statement} & \multicolumn{3}{c|}{Later Statement} & \multirow{2}{*}{Loop-carried?} & \multirow{2}{*}{Kind of Dataflow} \\
\cline{2-7}
 & Line & Iteration & Access & Line & Iteration & Access & & \\
\hline
$a(i+1)$ & 20 & $i$   & read  & 10 & $i+1$ & write & yes & anti \\
\hline
$b(i)$   & 30 & $i$   & read  & 20 & $i+1$ & write & yes & anti \\
\hline
$c(2)$   & 40 & $i$   & write & 30 & $i+1$ & read  & yes & flow \\
\hline
\end{tabular}}
\end{table}


\subsubsection*{Different parallelization strategies}

Other approaches were considered but not implemented for this project.
\emph{Task parallelization} was deemed unsuitable due to the theoretical inefficiency of the EM algorithm when decomposed into discrete tasks.
In fact, we should broadcast the whole dataset to all units for each task, since each task needs to access all data points to compute.
Then the overhead of managing multiple tasks and synchronizing their results (due to the EM algorithm's iterative nature) would outweigh any potential benefits.
\emph{Pipeline parallelization} was not applicable, as the EM algorithm's iterative nature does not lend itself to a linear sequence of processing stages.

%\emph{Model parallelization}: splitting the set of Gaussian components across devices when $K$ is large; each unit holds a subset of parameters and participates in responsibility computation via broadcast/gather.


