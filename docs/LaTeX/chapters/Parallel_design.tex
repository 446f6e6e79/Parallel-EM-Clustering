\section{Parallel Design}
Current state-of-the-art parallelization techniques for the EM algorithm primarily focus on data parallelism approaches, where
the dataset is partitioned across multiple processing units so that computations can be performed simultaneously on different subsets of the data.
\cite{altinigneli2013massively} present a GPU-based implementation of the EM algorithm that leverages the massive parallelism of GPUs to accelerate the computation of both the E-step and M-step using Asynchronous Model Update strategy.
Similarly, \cite{lee2016simple} propose a simple multithreaded version of the EM algorithm for finite mixture models.
\\Both approaches demonstrate significant speedups compared to traditional sequential implementations, making them suitable for large-scale data analysis tasks.

\subsection{Our Approach}
Building on the insights of the reviewed literature, our approach adopts a data-parallelism strategy to enhance the performance of the EM algorithm for GMMs.

\subsubsection*{E-step Parallelization}
The E-step is trivially parallelizable, as responsibilities for each data point are computed independently of each other.
Each processing unit can execute the standard E-step on its local data chunk, storing the results for the subsequent
M-step aggregation.

\subsubsection*{M-step Parallelization}
Differently, the M-step requires a more coordinated approach to ensure globally consistent parameters updates.
Each processing unit computes local accumulators for the sufficient statistics (i.e., effective counts, weighted sums for the means, and weighted squared differences for the variances) based on its local responsibilities.
These local results are then aggregated, in order to finalize the global parameter updates.

\subsubsection*{Clustering Assignment}
The clustering assignment step can be parallelized in a similar manner to the E-step.
Each processing unit determines the cluster assignments for its local data chunk using the final responsibilities from the preceding E-step.
Since the assignments are independent for each data point, this step requires no inter-process communication.
Once all units complete their local clustering assignments, the results can be gathered if needed for further analysis or output.

\subsubsection*{Performance Considerations}
Theoretically, this design reduces the computation times associated with the dataset size. Based on the computational complexity analysis in Section \ref{sec:complexity_analysis}, ideally the overall time complexity of the EM algorithm is reduced from $O(NKD)$ to $O\left(\frac{NKD}{P}\right)$, where $P$ is the number of processing units.
\\We assume that the number of clusters $K$ and dimensionality $D$ are significantly smaller than the number of data points $N$, which is a common characteristic of many real-world large-scale clustering applications.
Under this assumption, the computational cost is dominated by $N$, making our parallelization strategy particularly effective: distributing the data across processing units yields near-linear reductions in execution time.
However, as the number of processing units $P$ increases, the benefits of parallelization may diminish due to the communication overhead required for the M-step, which can outweigh the computational gains. 
This limits the scalability of the approach and reduces its efficiency, resulting in a bottleneck.
Moreover, the M-step remains manageable, as the size of the parameters that need to be communicated depends only on $K$ and $D$.
\subsection{Hybrid Parallelism}
An hybrid parallelization strategy could, in principle, mitigate some of the problems observed in pure data-parallel design.
Since the size of the of data exchanged during inter-process communication depends only on $K$ and $D$, reducing the number of processes while increasing the number of threads per process can help alleviate the communication overhead.
In such a configuration, each process operates on a larger portion of the dataset, allowing more computation to occur within a shared-memory environment, where threads can efficiently cooperate without the need for expensive message passing.
As a result, both the frequency and volume of inter-process communication taking place during the M-step can be significantly reduced.
\subsubsection{Data dependencies}
To assess the feasibility and potential benefits of hybrid parallelism within our implementation, 
we first examined the data dependencies inherent to the EM algorithm.

%#TODO: Reason if the single line comments should be interpreted as atomic or not.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.05}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Memory Location} & \multicolumn{3}{c|}{Earlier Statement} & \multicolumn{3}{c|}{Later Statement} & \multirow{2}{*}{Loop-carried?} & \multirow{2}{*}{Kind of Dataflow} \\
\cline{2-7}
 & Line & Iteration & Access & Line & Iteration & Access & & \\
\hline
\texttt{log\_likelihood} & 3 & $i$ & write & 22 & $i$ & read & no & flow \\
\hline
\texttt{log\_likelihood} & 22 & $i$ & write & 22 & $i + 1$ & read & yes & flow \\
\hline
\texttt{denom} & 15 & $i,k$   & write  & 15 & $i,k+1$ & read & yes & flow \\
\hline
\texttt{gamma}$(i * k + k)$   & 13 & $i,k$   & write  & 15 & $i,k$ & read & no & flow \\
\hline
\texttt{gamma}$(i * k + k)$   & 15 & $i,k$   & read  & 20 & $i,k$ & write & no & anti \\
\hline
\texttt{denom} & 15 & $i,k$   & write  & 20 & $i,k$ & read & no & flow \\
\hline
\texttt{denom} & 22 & $i$   & read  & 6 & $i+1$ & write & yes & anti \\
\hline
\texttt{denom} & 6 & $i$   & write  & 15 & $i,k$ & read & no & flow \\
\hline
\end{tabular}}
\end{table}

%M-step
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.05}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Memory Location} & \multicolumn{3}{c|}{Earlier Statement} & \multicolumn{3}{c|}{Later Statement} & \multirow{2}{*}{Loop-carried?} & \multirow{2}{*}{Kind of Dataflow} \\
\cline{2-7}
 & Line & Iteration & Access & Line & Iteration & Access & & \\
\hline
\texttt{gamma} & e-step $\rightarrow$ 20 & $i,k$ & write & 8 & $i,k$ & read & no & flow \\
\hline
\texttt{N\_k} & 8 & $i,k$ & write & 8 & $i+1,k$ & read & yes & flow \\
\hline
\texttt{mu\_k} & 11 & $i,k,d$ & write & 11 & $i+1,k,d$ & read & yes & flow \\
\hline
\texttt{N\_k} & 8 & $i,k$ & write & 19 & $k$ & read & no & flow \\
\hline
\texttt{mu\_k} & 11 & $i,k,d$ & write & 19 & $k,d$ & read & no & flow \\
\hline
\texttt{mu} & 19 & $k,d$ & write & 28 & $k,d$ & read & no & flow \\
\hline
\texttt{sigma\_k} & 30 & $k,d$ & write & 38 & $k,d$ & read & no & flow \\
\hline
\end{tabular}}
\end{table}
%#TODO: comment on sigma, mu, pi dependencies between M-step and E-step

\subsubsection*{Different parallelization strategies}

Other parallelization approaches were considered but ultimately not implemented for this project.
\\\emph{Task parallelization} was deemed unsuitable due to the inherent characteristics of the EM algorithm. Each task 
would require access to the entire dataset in order to compute the responsibilities and update the parameters.
This would introduce substantial communication overhead, effectively negating any potential performance gains.
Moreover, the iterative nature of the EM algorithm would require frequent synchronization between tasks, further increasing the overhead.
\\\emph{Pipeline parallelization} was not applicable. The EM algorithm does not follow an independent sequence of stages that can be overlapped in a pipeline fashion.
Instead it alternates expectation and maximization phases, each dependent on the results of the previous one.
This sequential dependency prevents the effective use of pipelining.

%\emph{Model parallelization}: splitting the set of Gaussian components across devices when $K$ is large; each unit holds a subset of parameters and participates in responsibility computation via broadcast/gather.


