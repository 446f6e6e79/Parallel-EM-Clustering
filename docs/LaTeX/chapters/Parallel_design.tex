\section{Parallel Design}
Current state-of-the-art parallelization techniques for the EM algorithm primarily focus on data parallelism approaches, where
the dataset is partitioned across multiple processing units so that computations can be performed simultaneously on different subsets of the data.
\cite{altinigneli2013massively} present a GPU-based implementation of the EM algorithm that leverages the massive parallelism of GPUs to accelerate the computation of both the E-step and M-step using Asynchronous Model Update strategy.
Similarly, \cite{lee2016simple} propose a simple multithreaded version of the EM algorithm for finite mixture models.
\\Both approaches demonstrate significant speedups compared to traditional single-threaded implementations, making them suitable for large-scale data analysis tasks.

\subsection{Our Approach}
Building on the insights of the reviewed literature, our approach adopts a data-parallelism strategy to enhance the performance of the EM algorithm for GMMs.
The dataset is partitioned into chunks and distributed across multiple processing units.
Each processing unit independently performs the E-step on its assigned data chunk, while the global parameters updates of the M-step are obtained by aggregating the partial results across all units.

\subsubsection*{E-step Parallelization}
The E-step is trivially parallelizable, as responsibilities for each data point are computed independently. 
Each processing unit executes the standard E-step on its local data chunk, storing the results for the subsequent 
M-step aggregation. This requires minimal modification from the sequential algorithm (Algorithm \ref{e_step_alg}).

\subsubsection*{M-step Parallelization}
Differently, the M-step requires a more coordinated approach to ensure globally consistent cluster parameters updates.
Each processing unit computes local accumulators for the sufficient statistics (i.e., effective counts, weighted sums for the means, and weighted squared differences for the variances) based on its local responsibilities.
These local results are then aggregated, in order to finalize the parameter updates.
\\\label{M_step_communication}Although the communication overhead in the M-step is relatively high with respect to the E-step, it remains
limited, since only the sufficient statistics need to be communicated.
Consequently, the communication cost is manageable and does not significantly impact the overall performance of the algorithm.

\subsubsection*{Clustering Assignment}
The clustering assignment step can also be parallelized in a similar manner to the E-step.
Each processing unit computes the cluster assignments for its local data chunk based on the final responsibilities computed in the last E-step.
Since the assignments are independent for each data point, this step can be executed in parallel without any inter-process communication.
After all processing units complete their local clustering assignments, the results can be gathered if needed for further analysis or output.

\subsubsection*{Performance Considerations}
Theoretically, this design reduces the computation times associated with the dataset size. As shown in the computational complexity analysis in Section \ref{sec:complexity_analysis}, the overall time complexity of the EM algorithm is reduced from $O(NKD)$ to $O\left(\frac{NKD}{P}\right)$, where $P$ is the number of processing units.
\\We assume that the number of clusters $K$ and dimensionality $D$ are significantly smaller than the number of data points $N$, which is a common characteristic of many real-world large-scale clustering applications.
Under this assumption, the computational cost is dominated by $N$, making our parallelization strategy particularly effective: distributing the data across processing units yields near-linear reductions in execution time.
Furthermore, this assumption ensures that the communication overhead during the M-step aggregation remains manageable, as the size of the parameters to be communicated between processing units depends only on $K$ and $D$.

\subsection{Hybrid Parallelism}
\subsubsection{Data dependencies}

\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.05}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Memory Location} & \multicolumn{3}{c|}{Earlier Statement} & \multicolumn{3}{c|}{Later Statement} & \multirow{2}{*}{Loop-carried?} & \multirow{2}{*}{Kind of Dataflow} \\
\cline{2-7}
 & Line & Iteration & Access & Line & Iteration & Access & & \\
\hline
$a(i+1)$ & 20 & $i$   & read  & 10 & $i+1$ & write & yes & anti \\
\hline
$b(i)$   & 30 & $i$   & read  & 20 & $i+1$ & write & yes & anti \\
\hline
$c(2)$   & 40 & $i$   & write & 30 & $i+1$ & read  & yes & flow \\
\hline
\end{tabular}}
\end{table}