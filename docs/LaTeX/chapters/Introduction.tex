\section{Introduction}

In this work, we address the problem of clustering large datasets using the
Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMMs).
Clustering is a fundamental task in unsupervised machine learning, where the goal is to group
similar data points together based on their inherent characteristics.
The EM algorithm is an iterative procedure for estimating the parameters of GMMs, enabling the 
identification of latent cluster structures within the data. However, the EM algorithm can be computationally intensive,
especially for large datasets, which motivates the need for parallelization to improve efficiency and scalability.
In this report, we present a parallel implementation of the EM algorithm for clustering, using a hybrid approach based on MPI and OpenMP.
We evaluate the performance of our implementation on various settings, analyzing its scalability, speedup, and efficiency.
Our results demonstrate the effectiveness of parallelization in accelerating the EM algorithm for clustering large datasets.

\subsection{Problem Overview}
The goal of clustering is to partition a dataset $\mathcal{X}=\{\boldsymbol{x}_1,\boldsymbol{x}_2,\dots, \boldsymbol{x}_N\}$ where $\boldsymbol{x}_i \in \mathbb{R}^D$ 
into $K$ (number of clusters) groups without using label information. 
Each cluster is represented by a probability distribution, and in our case, they are modeled as a Gaussian distribution.
The challenge is to estimate the parameters of these distributions-namely the means, covariances, and mixture weights- 
such that the overall likelihood of the data given the model is maximized.

\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.528\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../media/first_iteration.png}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.472\textwidth}
        \centering
        \includegraphics[width=\linewidth]{../media/em_visualization.png}
    \end{minipage}\hfill
    \caption{Example of clustering using GMMs with $K=3$ on a 2D dataset.}
    \label{fig:gmm_clusters}
\end{figure}

\subsection{Gaussian Mixture Models}

A Gaussian Mixture Model (GMM) \cite{Reynolds2009} is a probabilistic model that represents data as a combination
 of multiple Gaussian distributions, each with its own mean, covariance, and mixture weight. GMMs are widely used for clustering tasks due to their flexibility in modeling complex data distributions,
 where data points may naturally cluster around multiple centers with varying shapes and sizes.
So the goal is to estimate, by knowing the parameters of both the Gaussian components, to assign data points to these clusters.

A GMMs assumes that each data point 
is generated from one of $K$ Gaussian components:

\begin{equation}
    p(\boldsymbol{x} \mid \theta) = \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \Sigma_k)
    \label{generation_equation}
\end{equation}

where the sum of the mixture weights satisfies the following constraints:
\[
\sum_{k=1}^K \pi_k = 1, \quad \pi_k \ge 0
\]

\noindent
Here, $\pi_k$ are the mixture weights, $\boldsymbol{\mu}_k$ are the means, and $\Sigma_k$ are the covariance matrices of the Gaussian components.
The mixture weights $\pi_k$ represent the prior probability of component $k$, while the Gaussian density function $\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \Sigma_k)$
models the distribution of data points assigned to that component.
The Gaussian density function is defined as:

\begin{equation}
\mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \Sigma_k)
= \frac{1}{(2\pi)^{D/2} |\Sigma_k|^{1/2}}
  \exp\left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu}_k)^T 
  \Sigma_k^{-1} (\boldsymbol{x} - \boldsymbol{\mu}_k) \right)
  \label{gaussian_density}
\end{equation}

With this function, we can evaluate the likelihood of a data point $\boldsymbol{x}$ belonging to the $k$-th Gaussian component.

In our implementation we adopt a diagonal covariance matrix \cite{bishop2006pattern} for each Gaussian component. 
This assumption implies that the features are uncorrelated within each component, simplifying the covariance matrix to a diagonal form:


\[
\Sigma_k = 
\begin{pmatrix}
\sigma_{k,1} & 0 & \cdots & 0 \\
0 & \sigma_{k,2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{k,D}
\end{pmatrix}
\]

Then the Gaussian density function simplifies to:

\begin{equation}
  \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \Sigma_k)
= \frac{1}{(2\pi)^{D/2} \prod_{d=1}^D \sigma_{k,d}^{1/2}}
  \exp\Bigg(-\frac{1}{2} \sum_{d=1}^D \frac{(x_d - \mu_{k,d})^2}{\sigma_{k,d}}\Bigg)
  \label{diagonal_gaussian_density}
\end{equation}


\noindent
This simplification reduces computational complexity and memory usage,
making it feasible to work with high-dimensional data. In fact, evaluating the Gaussian density in the full-covariance 
case requires $O(D^3)$ time due to matrix inversion and determinant 
computation, while the diagonal formulation reduces this to $O(D)$. 

At this point, our main problem is to estimate the parameters $\theta = \{\pi_k, \boldsymbol{\mu}_k, \Sigma_k\}_{k=1}^K$ of the GMM given the dataset $\mathcal{X}$,
as they are unknown in our case.

\subsection{Expectation-Maximization algorithm} 

The Expectation-Maximization (EM) algorithm \cite{dempster1977em} is an iterative method for finding maximum likelihood estimates of parameters in statistical models,
where the model depends on unobserved latent variables. In the context of GMMs, the latent variables represent the cluster assignments of data points.
The EM algorithm consists of two main steps: the Expectation step (E-step) and the Maximization step (M-step).

\subsubsection*{E-Step}

Using the current parameter estimates $\theta$, we compute the posterior probabilities, also called responsibilities, that each data point belongs to each Gaussian component.
At the first iteration, the parameters are initialized randomly, in order to start the algorithm.
The responsibilities $\gamma_{ik}$, representing the probability that data point $\boldsymbol{x}_i$ belongs to cluster $k$ is computed as follows:

\begin{equation}
\gamma_{ik} 
= p(z_i = k \mid \boldsymbol{x}_i, \theta)
= \frac{\pi_k \, \mathcal{N}(\boldsymbol{x}_i \mid \boldsymbol{\mu}_k, \Sigma_k)}
       {\sum_{j=1}^{K} \pi_j \, \mathcal{N}(\boldsymbol{x}_i \mid \boldsymbol{\mu}_j, \Sigma_j)}
    \label{e_step_equation}
\end{equation}
\subsubsection*{M-Step}
Using the responsibilities \(\gamma_{ik}\) computed in the E-step, we update the parameters of the GMM to maximize the expected complete-data log-likelihood.
The updates for each component \(k = 1, \dots, K\) are as follows:

\begin{equation}
N_k = \sum_{i=1}^{N} \gamma_{ik}
\label{m_step_equation}
\end{equation}

Update of means:
\[\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} \boldsymbol{x}_i\]   

Update of covariance matrices:
\[\Sigma_k = \frac{1}{N_k} \sum_{i=1}^{N} \gamma_{ik} (\boldsymbol{x}_i - \boldsymbol{\mu}_k)(\boldsymbol{x}_i - \boldsymbol{\mu}_k)^T\]

Update of mixture weights:
\[\pi_k = \frac{N_k}{N}\]

After updating the parameters, the E-step and M-step are repeated until convergence. Convergence is typically assessed by monitoring
the change in log-likelihood between successive iterations. The process is considered converged if the change is below a predefined threshold $\epsilon$.
Alternatively, a maximum number of iterations can be set to prevent infinite loops.

The log-likelihood of the data given the model parameters is computed as:
\begin{equation}
\mathcal{L}(\theta ) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k \, \mathcal{N}(\boldsymbol{x}_i \mid \boldsymbol{\mu}_k, \Sigma_k) \right)
\label{log_likelihood}
\end{equation}

\subsubsection*{Clustering Assignment}

In order to cluster the dataset \cite{kak2024clustering} using the EM algorithm for GMMs, we simply assign each data point to the cluster corresponding 
to the Gaussian component with the highest responsibility. This is only done after the algorithm has converged or reached the maximum number of iterations.

\begin{equation}
\hat{z}_i = \arg \max_{k} p(z_i = k \mid \boldsymbol{x}_i, \theta) = \arg\max_{k} \gamma_{ik}
\label{clustering_equation}
\end{equation}

\subsubsection*{Pseudocode}

In the following, we provide pseudocode for the overall EM algorithm for GMMs.

\begin{algorithm}[H]
\caption{Expectation-Maximization for GMM clustering}
\begin{algorithmic}[1]
\State Initialize parameters $\{\pi_k, \mu_k, \Sigma_k\}_{k=1}^K$
\For{i $= 1 \dots \text{MAX\_ITER}$}
    \State \textbf{E-step:} compute responsibilities using \eqref{e_step_equation}
    \State \textbf{M-step:} update parameters using \eqref{m_step_equation}
    \State Compute log-likelihood $\mathcal{L}_{\text{curr}}$ according to \eqref{log_likelihood}
    \If{Converged} 
        \State break 
    \EndIf
\EndFor
\State \textbf{Clustering:} assign labels using \eqref{clustering_equation}
\end{algorithmic}
\label{em_alg}
\end{algorithm}


\subsection{Computational Complexity analysis}
\label{sec:complexity_analysis}
The computational cost of the EM algorithm is determined by the number of data points $N$, clusters $K$, and feature dimensions $D$.  
Both the E-step and M-step require $O(NKD)$ operations per iteration, resulting in an overall per-iteration complexity of $O(NKD)$.  
For large datasets, this can become a significant bottleneck, motivating the need for parallelization to improve its efficiency.






