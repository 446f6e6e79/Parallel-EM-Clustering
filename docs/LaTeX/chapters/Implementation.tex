\section{Implementation}

The implementation of the algorithm was carried out using C, inspired by the Python code provided in \cite{Azizi2023ParallelEM}.
The parallelization was achieved using MPI to enable distributed computing across multiple processors, while hybrid parallelization was achieved by integrating OpenMP to exploit multi-threading within each process.
In addition, Python and Bash scripts were developed to support systematic validation and analysis of the performance of our implementation.

\subsubsection*{Data generation}
Synthetic datasets were generated in Python to produce GMM-distributed data points, allowing full control over the number of clusters, dimensionality, and sample size.
The generator also provided metadata about the generated samples, along with the true labels for each data point. 

\subsubsection*{Accuracy Checks}
To ensure the correctness of the implementation, especially during initial testing we performed extensive validation checks 
by comparing the outputs of the algorithm against the real labels provided by the data generator.
Clustering performance were evaluated using permutation-invariant accuracy metrics (using the Python library \emph{scikit-learn}), which account for the arbitrary nature of cluster labeling in unsupervised learning.
By resolving label permutations, we could accurately measure how well the algorithm clustered the data points according to their true underlying distributions.

\subsection{Parallel Algorithm Implementation}

\subsubsection*{Data distribution}
The dataset, together with its metadata, is initially loaded by the root process (rank 0).
First, the metadata (number of samples, dimensions, and number of clusters) is broadcast to all processes using \emph{MPI\_Bcast}.
Once the metadata is known, each process computes the size of its local data chunk, which is then used to distribute the dataset across processes via \emph{MPI\_Scatterv}. 
After receiving their respective chunks, all processes proceed with the computation.

At the end of the execution (specifically after the clustering assignment step) each process holds the predicted labels for its local data chunk.
These local predicted labels are gathered back to the root process using \emph{MPI\_Gatherv}, in order to be written back to a file for accuracy evaluation.

\subsubsection*{E-step Implementation}
The implementation of the E-step is inherently data-parallel.
This requires minimal modification from the sequential algorithm (Algorithm \ref{e_step_alg}) and involves little to no inter-process communication.
The key implementation detail is that the log-likelihood, used for the convergence check, is computed during the responsibility calculation.
This avoids the need for a separate computation.

\begin{algorithm}[H]
\caption{E-step: Computation of Responsibilities}
\begin{algorithmic}[1]
\Require Dataset $X \in \mathbb{R}^{N \times D}$, parameters $\mu_{k}, \sigma_{k}, \pi_{k}$ for $k=1,\dots,K$
\Ensure Responsibilities $\gamma_{ik}$
\For{$i = 1$ to $N$}
    \State $\text{denom} \gets 0$
    \For{$k = 1$ to $K$}
        \State $\gamma_{ik}  \gets \pi_k \cdot \mathcal{N}(X[i] \mid \mu_k, \Sigma_k)$ \Comment{Gaussian density using \ref{gaussian_alg}}
        \State $\text{denom} \gets \text{denom} + \gamma_{ik}$
    \EndFor

    \For{$k = 1$ to $K$}
        \State $\gamma_{ik} \gets \gamma_{ik} / \text{denom}$ 
   \EndFor
\EndFor
\end{algorithmic}
\label{e_step_alg}
\end{algorithm}

\subsubsection*{M-step Implementation}
The M-step is implemented as outlined in Algorithm \ref{parallel_m_step}.
Each process first initializes local accumulators for the sufficient statistics and computes their partial distribution.
Upon completion, the local results are aggregated across all processes via \emph{MPI\_Allreduce}, which is preferred over
\emph{MPI\_Reduce} followed by a broadcast, since the combined results are needed by all processes for the subsequent steps.
Finally, each process updates the model parameters using the aggregated statistics.

\begin{algorithm}[H]
\caption{Parallel M-step}
\begin{algorithmic}[1]
\Require Local data chunk $X_{\text{loc}} \in \mathbb{R}^{N_{\text{loc}} \times D}$, local responsibilities $\gamma^{\text{loc}}_{ik}$
\Ensure Updated parameters $\mu_k$, $\sigma_k$, $\pi_k$

\State \textbf{Initialize local accumulators:}
\State $N^{\text{loc}}_k \gets 0$, \quad $\mu^{\text{loc}}_{kd} \gets 0$, \quad $\sigma^{\text{loc}}_{kd} \gets 0$

\Statex
\State \textbf{Local accumulation of} $N_k^{\text{loc}}$ \textbf{and weighted means} $\mu^{\text{loc}}_{kd}$
\For{$i = 1$ to $N_{\text{loc}}$}
    \For{$k = 1$ to $K$}
        \State $N^{\text{loc}}_k \gets N^{\text{loc}}_k + \gamma^{\text{loc}}_{ik}$
        \For{$d = 1$ to $D$}
            \State $\mu^{\text{loc}}_{kd} \gets \mu^{\text{loc}}_{kd} + \gamma^{\text{loc}}_{ik}\, X_{\text{loc}}[i,d]$
        \EndFor
    \EndFor
\EndFor
\State \textbf{AllReduce for} $N_k$ \textbf{and mean accumulators} $\mu^{acc}_{kd}$

\Statex
\State \textbf{Finalize means} $\mu_{kd}$
\For{$k = 1$ to $K$}
    \If{$N_k \le 0$} \State $N_k \gets \varepsilon$ \EndIf
    \For{$d = 1$ to $D$}
        \State $\mu_{kd} \gets \mu^{acc}_{kd} / N_k$
    \EndFor
\EndFor

\Statex

\State \textbf{Local accumulation of variance  $\sigma^{\text{loc}}_{kd}$ numerators}
\For{$i = 1$ to $N_{\text{loc}}$}
    \For{$k = 1$ to $K$}
        \For{$d = 1$ to $D$}
            \State $\sigma^{\text{loc}}_{kd} \gets \sigma^{\text{loc}}_{kd} 
             + \gamma^{\text{loc}}_{ik} \cdot (X_{\text{loc}}[i,d] - \mu_{kd})^2$
        \EndFor
    \EndFor
\EndFor
\State \textbf{AllReduce for variance accumulators} $\sigma^{acc}_{kd}$
\Statex
\State \textbf{Finalize variances and mixture weights} $\sigma_{kd}$, $\pi_k$
\For{$k = 1$ to $K$}
    \For{$d = 1$ to $D$}
        \State $\sigma_{kd} \gets \sigma^{acc}_{kd} / N_k$
    \EndFor
    \State $\pi_k \gets N_k / N$
\EndFor

\end{algorithmic}
\label{parallel_m_step}
\end{algorithm}

\subsection{Hybrid Parallel Algorithm Implementation}