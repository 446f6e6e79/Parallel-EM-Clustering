\section{Implementation}

The implementation of the algorithm was carried out using C, inspired by the Python code provided in \cite{Azizi2023ParallelEM}.
The parallelization was achieved using MPI to enable distributed computing across multiple processors, while hybrid parallelization was achieved by integrating OpenMP to exploit multi-threading within each process.
In addition, Python and Bash scripts were developed to support systematic validation and analysis of the performance of our implementation.

\subsubsection*{Data generation}
Synthetic datasets were generated in Python to produce GMM-distributed data points, allowing full control over the number of clusters, dimensionality, and sample size.
The generator also provided metadata about the generated samples, along with the true labels for each data point. 

\subsubsection*{Accuracy Checks}
To ensure the correctness of the implementation, especially during initial testing we performed extensive validation checks 
by comparing the outputs of the algorithm against the real labels provided by the data generator.
Clustering performance were evaluated using permutation-invariant accuracy metrics (Hungarian algorithm), which account for the arbitrary nature of cluster labeling in unsupervised learning.
By resolving label permutations, we could accurately measure how well the algorithm clustered the data points according to their true underlying distributions.

\subsection{Parallel Algorithm Implementation}

\subsubsection*{Data distribution}
The dataset, together with its metadata, is initially loaded by the root process (rank 0).
First, the metadata (number of samples, dimensions, and number of clusters) is broadcast to all processes using \emph{MPI\_Bcast}.
Once the metadata is known, each process computes the size of its local data chunk, which is then used to distribute the dataset across processes via \emph{MPI\_Scatterv}. 
After receiving their respective chunks, all processes proceed with the computation.

At the end of the execution (specifically after the clustering assignment step) each process holds the predicted labels for its local data chunk.
These local predicted labels are gathered back to the root process using \emph{MPI\_Gatherv}, in order to be written back to a file for accuracy evaluation.

%#TODO: rewrite this
\subsubsection*{E-step Implementation}
As said before, the parallelization of the E-step doesn't involve any change in the sequential algorithm.
The only notable difference, is that we compute the log-likelihood for the convergence check inside the E-step itself, in order to avoid additional computation.

\subsubsection*{M-step Implementation}
The M-step is implemented as outlined in Algorithm \ref{parallel_m_step}.
Each process first initializes local accumulators for the sufficient statistics and computes their partial distribution.
Upon completion, the local results are aggregated across all processes via \emph{MPI\_Allreduce}, which is preferred over
\emph{MPI\_Reduce} followed by a broadcast, since the combined results are needed by all processes for the subsequent steps.
Finally, each process updates the model parameters using the aggregated statistics.

\subsection{Hybrid Parallel Algorithm Implementation}